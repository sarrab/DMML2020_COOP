{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Last_Version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cl3JGH4MGv0"
      },
      "source": [
        "# A New Approach With Better Assumptions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgh9gs-5MPqk"
      },
      "source": [
        "Equipped with our previous analysis and subsequent findings, we came to the understanding that tweets are special kind of text which, thus, should be deal with in a particular way. Also, we realised that the logistic regression was giving the best results; hence, we will throughout this new approach focus mostly on it. Finally, from our previous initial EDA, word length seems to matter; we would like here to investigate further on it by making it one of the predictors.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hSDhcMEdt46"
      },
      "source": [
        "##Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoQHpm4dd286"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy\n",
        "\n",
        "from spacy import displacy\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPfjdYrEZ9pA"
      },
      "source": [
        "## Loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Y2ArCyy7Z9pC"
      },
      "source": [
        "\n",
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/sarrab/DMML2020_COOP/main/data/cleaned_data.csv\")\n",
        "df_test=pd.read_csv(\"https://raw.githubusercontent.com/sarrab/DMML2020_COOP/main/data/test_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_uWF0ULZ9rU"
      },
      "source": [
        "## Cleaning- Rethought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsuiKEwwO8if"
      },
      "source": [
        "This part is, actually, about our new approach towards cleaning tweets. You may find the code related under the same section in the notebook dedicated to cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ-8_lsRd9o5"
      },
      "source": [
        "### Feature creation: Word Average Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3id6YfpQIBb"
      },
      "source": [
        "This is from our initial EDA. As said, we are going to, in this part of our study, consider it. Instead of going with the whole word length, we thought trying a different approach towards the length of word by averaging it. Thus, this average word length will be along with the main text one of the predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "jFjK8RLsbV2D",
        "outputId": "5d789278-0240-4082-c577-4a431f2f58c3"
      },
      "source": [
        "\n",
        "def avg_word_length(x):\n",
        "    x = x.split()\n",
        "    return np.mean([len(i) for i in x])\n",
        "\n",
        "df_train['avg_word_length'] = df_train['text'].apply(avg_word_length)\n",
        "df_test['avg_word_length'] = df_test['text'].apply(avg_word_length)\n",
        "\n",
        "df_train.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>avg_word_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>black eye : a space battle occurred at star o ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>nolocation</td>\n",
              "      <td>&lt;hashtag&gt; world &lt;/hashtag&gt; fedex no longer to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5.235294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>reality training : train falls off elevated tr...</td>\n",
              "      <td>1</td>\n",
              "      <td>6.750000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     id  ... target avg_word_length\n",
              "0           0   3738  ...      0        4.812500\n",
              "1           1    853  ...      0        5.235294\n",
              "2           2  10540  ...      1        6.750000\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5id-idogQzm4"
      },
      "source": [
        "It can be either keep as such or be further normalised. We can use both the ways while modelling. That means, let us have normalised ones too. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6FqWW6PePh0"
      },
      "source": [
        "**Normalising the new feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Cd86RwC0bb3c",
        "outputId": "115af140-86ce-4040-d863-93f64831cc0e"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# transform data\n",
        "df_train['avg_word_length'] = scaler.fit_transform(df_train[['avg_word_length']])\n",
        "df_test['avg_word_length'] = scaler.fit_transform(df_test[['avg_word_length']])\n",
        "\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>avg_word_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>black eye : a space battle occurred at star o ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.308399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>nolocation</td>\n",
              "      <td>&lt;hashtag&gt; world &lt;/hashtag&gt; fedex no longer to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.352787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>reality training : train falls off elevated tr...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.511811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>&lt;hashtag&gt; taiwan &lt;/hashtag&gt; grace : expect tha...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.400767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>new isis video : isis threatens to behead croa...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.462234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>6466</td>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>&lt;hashtag&gt; earthquake &lt;/hashtag&gt; &lt;hashtag&gt; sism...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.255050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>6467</td>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>nolocation</td>\n",
              "      <td>&lt;user&gt; totally agree . she is and know what bi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.174103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>6468</td>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>nolocation</td>\n",
              "      <td>hollywood movie about trapped miners released ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.349081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>6469</td>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>BeirutToronto</td>\n",
              "      <td>friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.280840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>6470</td>\n",
              "      <td>1779</td>\n",
              "      <td>buildingsonfire</td>\n",
              "      <td>nolocation</td>\n",
              "      <td>buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.328084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0     id  ... target avg_word_length\n",
              "0              0   3738  ...      0        0.308399\n",
              "1              1    853  ...      0        0.352787\n",
              "2              2  10540  ...      1        0.511811\n",
              "3              3   5988  ...      1        0.400767\n",
              "4              4   6328  ...      1        0.462234\n",
              "...          ...    ...  ...    ...             ...\n",
              "6466        6466   4377  ...      1        0.255050\n",
              "6467        6467   3408  ...      0        0.174103\n",
              "6468        6468   9794  ...      1        0.349081\n",
              "6469        6469  10344  ...      1        0.280840\n",
              "6470        6470   1779  ...      1        0.328084\n",
              "\n",
              "[6471 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVhqMrofR381"
      },
      "source": [
        "This is how it looks like after being normalised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP0gsS7PeYcO"
      },
      "source": [
        "###Filling up the missing data With Tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZdpTzASGeb"
      },
      "source": [
        "The code for this workaround is available in the notebook dedicated to cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmLz4VFefZd9"
      },
      "source": [
        "###Ekphrasis: Dealing with Social Media Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWQjU2hWfqh_"
      },
      "source": [
        "Refer to the cleaning notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yujsC3ukfw3y"
      },
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1r8u4DeWs-b"
      },
      "source": [
        "From our previous models and assumptions, we did consider all the parameters as important and build our models taking them into account as predictors. In this new approach, we still have the same intuitions. In our earlier analysis when it came to using more than a single predictor, we merged them and then found their combined Tf-Idf.  \n",
        "Whereas, here, we thought generating the Tf-Idf of each predictor and then concatenating them and finally applying a dimensionality reduction technique, PCA, on the combined matrix. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAqwSSgxZYlC"
      },
      "source": [
        "##### Feature Selection- Train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYa5fOFoZ9rZ"
      },
      "source": [
        "# Select features\n",
        "X_txt = df_train['text'].values # the features we want to analyze\n",
        "X_loc = df_train['location'].values # the features we want to analyze\n",
        "X_keyw = df_train['keyword'].values # the features we want to analyze\n",
        "X_avg =  df_train['avg_word_length'].values\n",
        "\n",
        "y_train_dp = df_train['target'].values # the labels, or answers, we want to test against"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5UeS1gCZi-_"
      },
      "source": [
        "##### Feature Selection- Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhS5Wec6Z9rZ"
      },
      "source": [
        "# Select features\n",
        "X_txt_tst = df_test['text'].values # the features we want to analyze\n",
        "X_loc_tst = df_test['location'].values # the features we want to analyze\n",
        "X_keyw_tst = df_test['keyword'].values # the features we want to analyze\n",
        "X_avg_tst =  df_test['avg_word_length'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBKALOuHZ0dK"
      },
      "source": [
        "####Generating 3 tf-idf for the 3 columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSpz5iaTZ9ra"
      },
      "source": [
        "tfidf_vectorizer_txt = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "train_txt_tfidf = tfidf_vectorizer_txt.fit_transform(X_txt)\n",
        "tfidf_vectorizer_loc = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "train_loc_tfidf = tfidf_vectorizer_loc.fit_transform(X_loc.astype('U'))\n",
        "tfidf_vectorizer_kw = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "train_kw_tfidf = tfidf_vectorizer_kw.fit_transform(X_keyw.astype('U'))\n",
        "\n",
        "\n",
        "tfidf_vectorizer_Xtst = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "test_tfidf_txt = tfidf_vectorizer_Xtst.fit_transform(X_txt_tst)\n",
        "tfidf_vectorizer_loc_tst = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "test_tfidf_loc = tfidf_vectorizer_loc_tst.fit_transform(X_loc_tst.astype('U'))\n",
        "tfidf_vectorizer_kw_tst = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "test_tfidf_kw = tfidf_vectorizer_kw_tst.fit_transform(X_keyw_tst.astype('U'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eJqo1NbYwl"
      },
      "source": [
        "We do not need to vectorize the average word length since it is already a matrix of numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcKQJVeobBBy"
      },
      "source": [
        "####Concatenating Them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8b1tYT7Z9rc"
      },
      "source": [
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import hstack\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "diff_n_clmns = train_txt_tfidf.shape[0] - train_loc_tfidf.shape[0]\n",
        "diff_n_clmns_tst = test_tfidf_txt.shape[0] - test_tfidf_loc.shape[0]\n",
        "\n",
        "trainX_tfidf = scipy.sparse.vstack((train_loc_tfidf, csr_matrix((diff_n_clmns, train_loc_tfidf.shape[1]))))\n",
        "testX_tfidf = scipy.sparse.vstack((test_tfidf_loc, csr_matrix((diff_n_clmns_tst, test_tfidf_loc.shape[1]))))\n",
        "\n",
        "X_tfidf = hstack((train_txt_tfidf, trainX_tfidf))\n",
        "X_tfidf_tst = hstack((test_tfidf_txt, testX_tfidf))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUa4XWOpZ9rd"
      },
      "source": [
        "diff_n_clmns_1 = X_tfidf.shape[0] - train_kw_tfidf.shape[0]\n",
        "diff_n_clmns_1_tst = X_tfidf_tst.shape[0] - test_tfidf_kw.shape[0]\n",
        "\n",
        "trainX_tfidf_1 = scipy.sparse.vstack((train_kw_tfidf, csr_matrix((diff_n_clmns_1, train_kw_tfidf.shape[1]))))\n",
        "tstX_tfidf_1 = scipy.sparse.vstack((test_tfidf_kw, csr_matrix((diff_n_clmns_1_tst, test_tfidf_kw.shape[1]))))\n",
        "\n",
        "X_tfidf_1 = hstack((X_tfidf, trainX_tfidf_1))\n",
        "Xtst_tfidf_1 = hstack((X_tfidf_tst, tstX_tfidf_1))\n",
        "\n",
        "\n",
        "X_train_sparse = np.concatenate((X_tfidf_1.todense(), X_avg[:,None]), axis=1)\n",
        "X_test_sparse = np.concatenate((Xtst_tfidf_1.todense(), X_avg_tst[:,None]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQOEmhcZ9rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e2891d-4f99-493d-8d0b-4242e93caf17"
      },
      "source": [
        "X_train_sparse.shape "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6471, 15773)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1n6et0XZ9rd"
      },
      "source": [
        "## Classify After Deep Text Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFfoBINGZ9re"
      },
      "source": [
        "### PCA and Logistic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epi_Kuhadkri"
      },
      "source": [
        "It is evident that the results of generating all these Tf-Idfs will explode the features space. As a workaround, we have used PCA to reduce this dimensionality. Then we apply our logictic regression model.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlQHmzW1f4xB"
      },
      "source": [
        "#####Without Average Word length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OephwflvgJc3"
      },
      "source": [
        "In this first model, we did not use the average word length feature as a predictor.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mrz177HZ9re",
        "outputId": "82c84474-27c6-4957-a8aa-503b1e1b8d9e"
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "X_train_scaled = scaler.fit_transform(X_tfidf_1.todense())\n",
        "X_test_scaled = scaler.fit_transform(Xtst_tfidf_1.todense())\n",
        "\n",
        "\n",
        "# Define PCA\n",
        "pca = PCA(n_components=300)\n",
        "pca_1 = PCA(n_components=300)\n",
        "\n",
        "# Example on X_train_vec\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "print('Shape after PCA: ', X_train_pca.shape)\n",
        "print('Number of components: ', pca.n_components_)\n",
        "\n",
        "x_tst_pca = pca_1.fit_transform(X_test_scaled)\n",
        "print('Shape after PCA: ', x_tst_pca.shape)\n",
        "print('Number of components: ', pca_1.n_components_)\n",
        "\n",
        "\n",
        "# Fit model\n",
        "\n",
        "log_reg_s = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, random_state = 72)\n",
        "\n",
        "start = time.time()\n",
        "log_reg_s.fit(X_train_pca, y_train_dp)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "\n",
        "y_pred = log_reg_s.predict(x_tst_pca)\n",
        "y_pred\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after PCA:  (6471, 300)\n",
            "Number of components:  300\n",
            "Shape after PCA:  (1142, 300)\n",
            "Number of components:  300\n",
            "Time:  3.1056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brjbVW0Rgfjz"
      },
      "source": [
        "#####With All predictors: Average Word Length included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IzR3mzMguF_"
      },
      "source": [
        "Here we make use of the concatenated Tf-Ifd which includes average word length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwHY9c5Dpnzu",
        "outputId": "8ae46b1c-5a6a-4bf1-d59e-499d4b6f0418"
      },
      "source": [
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
        "X_test_scaled = scaler.fit_transform( X_test_sparse)\n",
        "\n",
        "\n",
        "# Define PCA\n",
        "pca = PCA(n_components=100)\n",
        "pca_1 = PCA(n_components=100)\n",
        "\n",
        "# Example on X_train_vec\n",
        "X_train_pca_1 = pca.fit_transform(X_train_scaled)\n",
        "print('Shape after PCA: ', X_train_pca_1.shape)\n",
        "print('Number of components: ', pca.n_components_)\n",
        "\n",
        "x_tst_pca_1 = pca_1.fit_transform(X_test_scaled)\n",
        "print('Shape after PCA: ', x_tst_pca_1.shape)\n",
        "print('Number of components: ', pca_1.n_components_)\n",
        "\n",
        "\n",
        "# Fit model\n",
        "\n",
        "log_reg_s = LogisticRegressionCV(solver='saga', cv=5, max_iter=1200, random_state = 72)\n",
        "\n",
        "start = time.time()\n",
        "log_reg_s.fit(X_train_pca_1, y_train_dp)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "\n",
        "y_pred_1 = log_reg_s.predict(x_tst_pca_1)\n",
        "y_pred_1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after PCA:  (6471, 100)\n",
            "Number of components:  100\n",
            "Shape after PCA:  (1142, 100)\n",
            "Number of components:  100\n",
            "Time:  89.7676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTn0YHUhE8k"
      },
      "source": [
        "#####With All predictors: Average Word Length included: With different Hyperparameters and more number of Principal Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgKNsD8A3bv9",
        "outputId": "2e8528f6-dcc7-41de-b053-d1a29c3eb7b3"
      },
      "source": [
        "#With only Location and Text\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "X_train_scaled_1 = scaler.fit_transform(X_tfidf.todense())\n",
        "X_test_scaled_1 = scaler.fit_transform(X_tfidf_tst.todense())\n",
        "\n",
        " \n",
        "\n",
        "# Define PCA\n",
        "pca_2 = PCA(n_components=150)\n",
        "pca_3 = PCA(n_components=150)\n",
        "\n",
        "# Example on X_train_vec\n",
        "X_train_pca_2 = pca_2.fit_transform(X_train_scaled_1)\n",
        "print('Shape after PCA: ', X_train_pca_2.shape)\n",
        "print('Number of components: ', pca_2.n_components_)\n",
        "\n",
        "x_tst_pca_2 = pca_3.fit_transform(X_test_scaled_1)\n",
        "print('Shape after PCA: ', x_tst_pca_2.shape)\n",
        "print('Number of components: ', pca_3.n_components_)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "log_reg_rdc_2 = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, random_state = 72)\n",
        "\n",
        "log_reg_rdc_2.fit(X_train_pca_2, y_train_dp)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "\n",
        "y_pred_real_rdc_2 = log_reg_rdc_2.predict(x_tst_pca_2)\n",
        "y_pred_real_rdc_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after PCA:  (6471, 150)\n",
            "Number of components:  150\n",
            "Shape after PCA:  (1142, 150)\n",
            "Number of components:  150\n",
            "Time:  2.168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfEnCmVhgyD"
      },
      "source": [
        "#CONCLUSIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7cqItaSl7QG"
      },
      "source": [
        "Given a chance to crosscheck the above predictions against the real ones, we strongly believe that our accuracies would have been improve. \n",
        "Yet, we could have, for instance, kept a part of our Trainset as a simulation of Test set.\n",
        "\n",
        "Another thing, we have realised, here, is that the chosen feature 'average word length' is not that meaningful. Because, after averaging both the distributions do not appear significantly different. Thus, it would have been better to use other features that we explored during our initial EDA, for instance, 'total character' or 'word length'.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_x3CTqWhjlx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}