{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "COOP20.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-HAoCD90Y5p"
      },
      "source": [
        "#I added some codes to the one by Jessy\n",
        "#I guess If I try to work on the latest version in the git -repository, everyone can see and work on the latest version."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRZJWMkm36oO",
        "outputId": "61c5f847-1096-48b4-c532-e8a28aa6a1c9"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkEPZvWrnbpS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,LinearRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIgLjnoPnhoO"
      },
      "source": [
        "# Import required packages\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# load English language model of spacy\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LERXZOhyxkfM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWUhyRH036oP"
      },
      "source": [
        "## Loading "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVjt2_vb36oP"
      },
      "source": [
        "Below is how the dataset looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpmYt5Xq43FX"
      },
      "source": [
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/project/training_data.csv\")\n",
        "df_test=pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/project/test_data.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_yg88TjI36oP"
      },
      "source": [
        "#df_train = pd.read_csv(\"/Users/MBP/Documents/UNIL/DM&ML/Project/82653de5-2ed1-4782-959e-23eba75d67a9_training_data (1).csv\")\n",
        "#df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoIGMffY36oR"
      },
      "source": [
        "There are 6471 rows and 5 columns in the training data.\n",
        "Since column text contains the tweets, our first assumption is that it might be the most important predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPoDvXi_36oS"
      },
      "source": [
        "### What does each column show?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EPRUH4F36oS"
      },
      "source": [
        "This dataset contains tweets about whether or not a reported disaster is real. \n",
        "Some of the features in the dataset are:\n",
        "\n",
        ">\t1.\t`Id` : id of the tweet.\n",
        ">\t2.\t`keyword`: a kind of a title for that paritucular disaster \n",
        ">\t3.\t`location`: the geographical location where that disaster happened.\n",
        ">\t4.\t`text`: the disaster messages in form a tweet.\n",
        ">\t5.\t`target`: value representing whether the reported tweet was true or not\n",
        "    \n",
        "Our task is to predict whether a tweet is real or not, i.e, we should use the target column as the labels.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4wS_XGf36oS"
      },
      "source": [
        "### A look at the Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axuGov8O36oS"
      },
      "source": [
        "#df_test =  pd.read_csv(\"/Users/MBP/Documents/UNIL/DM&ML/Project/5fc21f33-b209-4b07-ad70-a69020dfd2cf_test_data.csv\")\n",
        "#df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS5XLVnp36oS"
      },
      "source": [
        "And we observe that there are 1142 rows and 4 columns in the training data. \n",
        "Having four columns instead 5 here indicates that these are predictors and the column \"target\" is the predicted "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LfS6Xy536oS"
      },
      "source": [
        "# Initial EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoJDc8_36oS"
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttUPAXaY36oT"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge-UKdUo36oT"
      },
      "source": [
        "df_train_copy = df_train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMB7zn9v36oT"
      },
      "source": [
        "#### A quick look at the different types of tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50JNbBAV36oT"
      },
      "source": [
        "Below is a tweet on real disaster "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfAn4jNJ36oT"
      },
      "source": [
        "# a quick look on data\n",
        "quick_look_real = df_train[df_train[\"target\"] == 1 ]['text'].values[2]\n",
        "quick_look_real\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B3tu6lU36oT"
      },
      "source": [
        "This is a typical tweet on fake disaster "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFnRcqcH36oU"
      },
      "source": [
        "# a quick look on data\n",
        "quick_look_fake = df_train[df_train[\"target\"] == 0 ]['text'].values[2]\n",
        "quick_look_fake\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvVOvPbj36oU"
      },
      "source": [
        "#### Let's see how the real and fake disaster tweets are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2es_GCl36oU"
      },
      "source": [
        "df_train['target'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZZXOMdQ36oU"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.subplots(1,1,figsize=(10,8))\n",
        "sns.countplot('target',data=df_train) #this gives the frequency of each\n",
        "plt.title(\"Disaster tweets Distribution\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SpPm4nF36oU"
      },
      "source": [
        "It is clear that there are more fake than real ones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOhqxQDcHP6i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm56X-qx36oU"
      },
      "source": [
        "### The Base rate is given as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qlg0nEa36oU"
      },
      "source": [
        "base_rate=round(df_train.target.value_counts()[0]/len(df_train), 4 )\n",
        "print(f'\\nThe base rate is {base_rate}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La7gEG_c36oU"
      },
      "source": [
        "## Let's do further exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGmE3Oxm36oV"
      },
      "source": [
        "df_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3KNsDnz36oV"
      },
      "source": [
        "#### Words Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drkgz0Ck36oV"
      },
      "source": [
        "# let's split the words\n",
        "\n",
        "df_train['totalwords'] = df_train['text'].str.split().str.len()\n",
        "# df_train['totalwords'] = df_train['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "df_test['totalwords'] = df_test['text'].str.split().str.len()\n",
        "\n",
        "df_train['totalwords'].hist(edgecolor='black', linewidth=1.2)\n",
        "df_test['totalwords'].hist(edgecolor='black', linewidth=1.2)\n",
        "plt.title('Total number of Words Distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ifho--r9r0f"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSO6HrK17oMm"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bACPKln-36oV"
      },
      "source": [
        "This is how the total words are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySVCSvSa36oV"
      },
      "source": [
        "df_train['totalwords'][df_train['target'] == 0].plot.hist(bins=20)\n",
        "df_train['totalwords'][df_train['target'] == 1].plot.hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm0Ao0_I36oV"
      },
      "source": [
        "Here, we see that the most longest real disaster tweets have between 15 and 20 words.\n",
        "\n",
        "generally speaking, fake ones are more longer than the real ones. othervise, there is a similar central tendency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJMWfP8R36oV"
      },
      "source": [
        "#### Character Distribution Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVuWqMz36oV"
      },
      "source": [
        "Here, we compare the distribution of real and fake disaster tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U545X5jf36oV"
      },
      "source": [
        "df_train['totalcharacter'] = df_train['text'].apply(lambda x: len(x))\n",
        "df_train['totalcharacter'][df_train['target'] == 0]\n",
        "\n",
        "#Test\n",
        "df_test['totalcharacter'] = df_test['text'].apply(lambda x: len(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdc4IGyq36oV"
      },
      "source": [
        "df_train['totalcharacter'][df_train['target'] == 0].plot.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2aw0xGt36oW"
      },
      "source": [
        "df_train['totalcharacter'][df_train['target'] == 1].plot.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-XWhDsV36oW"
      },
      "source": [
        "df_train['totalcharacter'][df_train['target'] == 0].plot.hist()\n",
        "df_train['totalcharacter'][df_train['target'] == 1].plot.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCpXJr7K36oW"
      },
      "source": [
        "> We observe that real disaster tweet characters are fewer than the fakes ones.\n",
        "> Real disaster tweets have at least 20 characters. And most of them have at least 80 characters\n",
        "> This could help latter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIQpER4l36oW"
      },
      "source": [
        "#### unique word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HegxvTO536oW"
      },
      "source": [
        "df_train['uniqueword'] = df_train['text'].apply(lambda x: len(set(x.split())))\n",
        "df_train['uniqueword'][df_train['target'] == 0].plot.hist()\n",
        "df_train['uniqueword'][df_train['target'] == 1].plot.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phdffgu_36oW"
      },
      "source": [
        "If this feature 'unique word' is not too correlated with the other feature, we believe that it can be a good predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q7-S-Q-36oW"
      },
      "source": [
        "#### Average Word Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGOgv9W_36oW"
      },
      "source": [
        "def avg_word_length(x):\n",
        "    x = x.split()\n",
        "    return np.mean([len(i) for i in x])\n",
        "\n",
        "df_train['avg_word_length'] = df_train['text'].apply(avg_word_length)\n",
        "df_train['avg_word_length'][df_train['target'] == 0].plot.hist()\n",
        "df_train['avg_word_length'][df_train['target'] == 1].plot.hist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okowej6K36oW"
      },
      "source": [
        "We may use this feature during our feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-DUppX36oW"
      },
      "source": [
        "#### The location with the most fake disaster tweet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LAwFWBJ36oX"
      },
      "source": [
        "df_train['location'][df_train['target'] == 0].value_counts().plot(figsize=(15, 12))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGAdEzI-36oX"
      },
      "source": [
        ">According this most of the fake disaster tweets apparently come from New york.\n",
        ">And many have a pseudoname as location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY4hsPdk36oX"
      },
      "source": [
        "#### The location with the most real disaster tweet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbt0o-wy36oX"
      },
      "source": [
        "df_train['location'][df_train['target'] == 1].value_counts().plot(figsize=(15, 12))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXQlY7bK36oX"
      },
      "source": [
        "This gives an idea about the locations where most of the real tweets come from  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Qh8kV136oX"
      },
      "source": [
        "###### All these above can become new features to help us explore the attributes of tweets that are about disasters. \n",
        "###### If we think these new features are useful, we can add them as input our model with the text data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcgE9N5No0Kb"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiKdQPkW36oX"
      },
      "source": [
        "# A function to do some text cleaning\n",
        "\n",
        "def func_textcleaning(sentence):\n",
        "    # remove unicode literals\n",
        "    temp = sentence.encode('ascii',errors='ignore').decode('ascii')\n",
        "    \n",
        "    # remove &amp\n",
        "    temp = temp.replace('&amp;', '')\n",
        "    \n",
        "    # remove urls\n",
        "    temp = re.sub(r\"http\\S+\", \"\", temp)\n",
        "    \n",
        "    # remove html\n",
        "    temp = re.sub(r'<.*?>', \"\", temp)\n",
        "    \n",
        "    # remove hashtags\n",
        "    temp = re.sub(r'#', \"\", temp)\n",
        "\n",
        "    # remove people account with @\n",
        "    temp = re.sub(r'@\\S+', \"\", temp)\n",
        "    \n",
        "    return temp"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH7hjvP8pJ4t"
      },
      "source": [
        "df_train['text']=df_train.text.apply(lambda x:func_textcleaning(x))\n",
        "df_test['text']=df_test.text.apply(lambda x:func_textcleaning(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOkeFLW6pXKy"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2M0_XfgpiHy"
      },
      "source": [
        "# Create tokenizer function for preprocessing\n",
        "def spacy_tokenizer(text):\n",
        "\n",
        "    # Define stopwords, punctuation, and numbers\n",
        "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "    punctuations = string.punctuation\n",
        "    # numbers = \"0123456789\"\n",
        "\n",
        "    # Create spacy object\n",
        "    mytokens = sp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    \n",
        "    # Remove all word with less that 3 letters (remove noise)\n",
        "    mytokens = [ word for word in mytokens if len(word)>2 ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPzjMIifph6x"
      },
      "source": [
        "# Tokenize texts\n",
        "%%time\n",
        "processed_texts = []\n",
        "for text in df_train.text:\n",
        "    processed_text = spacy_tokenizer(text)\n",
        "    processed_texts.append(processed_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmycqTt1ppcv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT3hBF5-pp-d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDehEC2GppNc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vywBawug36oX"
      },
      "source": [
        "# MODELS\n",
        "# Initial Classification Model: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UwOR9_-36oX"
      },
      "source": [
        "Our initial assumption is that the bydault predictors (keyword, location and especially text) have a correlation or perhaps a causality with the fact that a tweet is either real or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TQYZrUFsZrM"
      },
      "source": [
        "# TF-IDF with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXzNVWWYsZJ0"
      },
      "source": [
        "# Select features\n",
        "X = df_train['text'] # the features we want to analyze\n",
        "y = df_train['target'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTVadxwAsvLk",
        "outputId": "46208f0b-e156-47dc-b660-41dfed3e2970"
      },
      "source": [
        "%%time\n",
        "# Define vectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), tokenizer=spacy_tokenizer)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=1000, cv=5)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 2s, sys: 112 ms, total: 1min 2s\n",
            "Wall time: 1min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfjEP-ZdvR18",
        "outputId": "b0090f09-092d-4003-caeb-fedfb5c352f6"
      },
      "source": [
        "#Train Accuracy  \n",
        "%%time\n",
        "pipe.score(X_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9289026275115919"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPA-JjENsu_P",
        "outputId": "8f451ae6-9c6e-4b4d-e1d5-e46185633cba"
      },
      "source": [
        "# Predictions and evaluation for the model\n",
        "%%time\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print(round(accuracy_score(y_test, y_pred), 4))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7876\n",
            "CPU times: user 12.6 s, sys: 3.01 ms, total: 12.6 s\n",
            "Wall time: 12.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IZKeCrrtHy8"
      },
      "source": [
        "Decision Tree Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCSnb1jztukc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hznxq5H3tuHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGvSfuV2tprE"
      },
      "source": [
        "# SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x79AYL_vt0ml"
      },
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf),\n",
        "                 ('classifier', classifier)])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5t_WupEubvU",
        "outputId": "f03b5aa5-d5a0-4377-b8dc-1c19ce33dd68"
      },
      "source": [
        "#fit with the train data: features and labels.\n",
        "pipe.fit(df_train.text, df_train.target)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7f6e8fbe8bf8>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegressionCV(Cs=10, class_weight=None, cv=5,\n",
              "                                      dual=False, fit_intercept=True,\n",
              "                                      intercept_scaling=1.0, l1_ratios=None,\n",
              "                                      max_iter=1000, multi_class='auto',\n",
              "                                      n_jobs=None, penalty='l2',\n",
              "                                      random_state=None, refit=True,\n",
              "                                      scoring=None, solver='lbfgs', tol=0.0001,\n",
              "                                      verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPhvndtkubgo",
        "outputId": "babde100-3bb6-4265-d4f1-aff5d27ae0a3"
      },
      "source": [
        "#Train Accuracy  \n",
        "pipe.score(df_train.text, df_train.target)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.923659403492505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrYyaCQFwvh7"
      },
      "source": [
        "#text label predictions:\n",
        "preds=pipe.predict(df_test.text)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3DyzaoTxHZ0"
      },
      "source": [
        "#Data frame and submission format\n",
        "y_pred=pd.DataFrame(preds)\n",
        "y_pred.columns=['target']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0A-4baqmxSFD",
        "outputId": "bc705521-acbd-4b21-b234-4e25175a5bfa"
      },
      "source": [
        "from google.colab import files\n",
        "y_pred.to_csv('coop22Nov.csv', index=False) \n",
        "files.download('coop22Nov.csv')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d035d835-a04c-486f-9cb0-66b8d87f7789\", \"coop22Nov.csv\", 2291)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOgeRW33tpOt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mbFt4LKe36oX"
      },
      "source": [
        "df_train_copy.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAuR3PiH36oX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqAPKhqI36oX"
      },
      "source": [
        "df_train.text.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XepzHgMT36oY"
      },
      "source": [
        "df_train.keyword.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klkrdfeU36oY"
      },
      "source": [
        "df_train.keyword.unique()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}