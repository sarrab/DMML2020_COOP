{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Last Version.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarrab/DMML2020_COOP/blob/master/Last_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cl3JGH4MGv0"
      },
      "source": [
        "# A New Approach With Better Assumptions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgh9gs-5MPqk"
      },
      "source": [
        "Equipped with our previous analysis and subsequent findings, we came to the understanding that tweets are special kind of text which, thus, should be deal with in a particular way. Also, we realised that the logistic regression was giving the best results; hence, we will throughout this new approach focus mostly on it. Finally, from our previous initial EDA, word length seems to matter; we would like here to investigate further on it by making it one of the predictors.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPfjdYrEZ9pA"
      },
      "source": [
        "# Loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Y2ArCyy7Z9pC"
      },
      "source": [
        "\n",
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/project/training_data.csv\")\n",
        "df_test=pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/project/test_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_uWF0ULZ9rU"
      },
      "source": [
        "## Cleaning- Rethought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsuiKEwwO8if"
      },
      "source": [
        "This part is, actually, about our new approach towards cleaning tweets. You may find the code related under the same section in the notebook dedicated to cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ-8_lsRd9o5"
      },
      "source": [
        "## Feature creation: Word Average Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3id6YfpQIBb"
      },
      "source": [
        "This is from our initial EDA. As said, we are going to, in this part of our study, consider it. Instead of going with the whole word length, we thought trying a different approach towards the length of word by averaging it. Thus, this average word length will be along with the main text one of the predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "jFjK8RLsbV2D",
        "outputId": "e8495d8c-b938-49c1-b751-cce7e6e8c320"
      },
      "source": [
        "\n",
        "def avg_word_length(x):\n",
        "    x = x.split()\n",
        "    return np.mean([len(i) for i in x])\n",
        "\n",
        "df_train_copy_9['avg_word_length'] = df_train_copy_9['text'].apply(avg_word_length)\n",
        "df_test_copy['avg_word_length'] = df_test_copy['text'].apply(avg_word_length)\n",
        "\n",
        "df_train_copy_9.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>avg_word_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "      <td>4.631579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "      <td>6.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "      <td>7.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    keyword  ... target avg_word_length\n",
              "0   3738  destroyed  ...      0        4.631579\n",
              "1    853  bioterror  ...      0        6.066667\n",
              "2  10540  windstorm  ...      1        7.833333\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5id-idogQzm4"
      },
      "source": [
        "It can be either keep as such or be further normalised. We can use both the ways while modelling. That means, let us have normalised ones too. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6FqWW6PePh0"
      },
      "source": [
        "**Normalising the new feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Cd86RwC0bb3c",
        "outputId": "42a034f0-f8ac-4c91-8d92-df3ad08d5b74"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# transform data\n",
        "df_train_copy_9['avg_word_length'] = scaler.fit_transform(df_train_copy_9[['avg_word_length']])\n",
        "df_test_copy['avg_word_length'] = scaler.fit_transform(df_test_copy[['avg_word_length']])\n",
        "\n",
        "df_train_copy_9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>avg_word_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.135191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.219608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.323529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.218782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.239908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.274510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.135686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.277970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.198039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings%20on%20fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                keyword  ... target avg_word_length\n",
              "0      3738              destroyed  ...      0        0.135191\n",
              "1       853              bioterror  ...      0        0.219608\n",
              "2     10540              windstorm  ...      1        0.323529\n",
              "3      5988              hazardous  ...      1        0.218782\n",
              "4      6328                hostage  ...      1        0.239908\n",
              "...     ...                    ...  ...    ...             ...\n",
              "6466   4377             earthquake  ...      1        0.274510\n",
              "6467   3408                 derail  ...      0        0.135686\n",
              "6468   9794                trapped  ...      1        0.277970\n",
              "6469  10344                weapons  ...      1        0.198039\n",
              "6470   1779  buildings%20on%20fire  ...      1        0.143288\n",
              "\n",
              "[6471 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVhqMrofR381"
      },
      "source": [
        "This is how it looks like after being normalised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP0gsS7PeYcO"
      },
      "source": [
        "###Filling up the missing data With Tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZdpTzASGeb"
      },
      "source": [
        "The code for this workaround is available in the notebook dedicated to cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmLz4VFefZd9"
      },
      "source": [
        "###Ekphrasis: Dealing with Social Media Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWQjU2hWfqh_"
      },
      "source": [
        "Refer to the cleaning notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yujsC3ukfw3y"
      },
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1r8u4DeWs-b"
      },
      "source": [
        "From our previous models and assumptions, we did consider all the parameters as important and build our models taking them into account as predictors. In this new approach, we still have the same intuitions. In our earlier analysis when it came to using more than a single predictor, we merged them and then found their combined Tf-Idf.  \n",
        "Whereas, here, we thought generating the Tf-Idf of each predictor and then concatenating them and finally applying a dimensionality reduction technique, PCA, on the combined matrix. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAqwSSgxZYlC"
      },
      "source": [
        "##### Feature Selection- Train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYa5fOFoZ9rZ"
      },
      "source": [
        "# Select features\n",
        "X_txt = df_train_copy_9['text'].values # the features we want to analyze\n",
        "X_loc = df_train_copy_9['location'].values # the features we want to analyze\n",
        "X_keyw = df_train_copy_9['keyword'].values # the features we want to analyze\n",
        "X_avg =  df_train_copy_9['avg_word_length'].values\n",
        "\n",
        "y_train_dp = df_train_copy_9['target'].values # the labels, or answers, we want to test against"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5UeS1gCZi-_"
      },
      "source": [
        "##### Feature Selection- Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhS5Wec6Z9rZ"
      },
      "source": [
        "# Select features\n",
        "X_txt_tst = df_test_copy['text'].values # the features we want to analyze\n",
        "X_loc_tst = df_test_copy['location'].values # the features we want to analyze\n",
        "X_keyw_tst = df_test_copy['keyword'].values # the features we want to analyze\n",
        "X_avg_tst =  df_test_copy['avg_word_length'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBKALOuHZ0dK"
      },
      "source": [
        "####Generating 3 tf-idf for the 3 columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSpz5iaTZ9ra"
      },
      "source": [
        "tfidf_vectorizer_txt = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "train_txt_tfidf = tfidf_vectorizer_txt.fit_transform(X_txt)\n",
        "tfidf_vectorizer_loc = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "train_loc_tfidf = tfidf_vectorizer_loc.fit_transform(X_loc.astype('U'))\n",
        "tfidf_vectorizer_kw = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "train_kw_tfidf = tfidf_vectorizer_kw.fit_transform(X_keyw.astype('U'))\n",
        "\n",
        "\n",
        "tfidf_vectorizer_Xtst = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "test_tfidf_txt = tfidf_vectorizer_Xtst.fit_transform(X_txt_tst)\n",
        "tfidf_vectorizer_loc_tst = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "test_tfidf_loc = tfidf_vectorizer_loc_tst.fit_transform(X_loc_tst.astype('U'))\n",
        "tfidf_vectorizer_kw_tst = TfidfVectorizer(use_idf = True, max_df = 0.95)\n",
        "test_tfidf_kw = tfidf_vectorizer_kw_tst.fit_transform(X_keyw_tst.astype('U'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eJqo1NbYwl"
      },
      "source": [
        "We do not need to vectorize the average word length since it is already a matrix of numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcKQJVeobBBy"
      },
      "source": [
        "####Concatenating Them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8b1tYT7Z9rc"
      },
      "source": [
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import hstack\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "diff_n_clmns = train_txt_tfidf.shape[0] - train_loc_tfidf.shape[0]\n",
        "diff_n_clmns_tst = test_tfidf_txt.shape[0] - test_tfidf_loc.shape[0]\n",
        "\n",
        "trainX_tfidf = scipy.sparse.vstack((train_loc_tfidf, csr_matrix((diff_n_clmns, train_loc_tfidf.shape[1]))))\n",
        "testX_tfidf = scipy.sparse.vstack((test_tfidf_loc, csr_matrix((diff_n_clmns_tst, test_tfidf_loc.shape[1]))))\n",
        "\n",
        "X_tfidf = hstack((train_txt_tfidf, trainX_tfidf))\n",
        "X_tfidf_tst = hstack((test_tfidf_txt, testX_tfidf))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUa4XWOpZ9rd"
      },
      "source": [
        "diff_n_clmns_1 = X_tfidf.shape[0] - train_kw_tfidf.shape[0]\n",
        "diff_n_clmns_1_tst = X_tfidf_tst.shape[0] - test_tfidf_kw.shape[0]\n",
        "\n",
        "trainX_tfidf_1 = scipy.sparse.vstack((train_kw_tfidf, csr_matrix((diff_n_clmns_1, train_kw_tfidf.shape[1]))))\n",
        "tstX_tfidf_1 = scipy.sparse.vstack((test_tfidf_kw, csr_matrix((diff_n_clmns_1_tst, test_tfidf_kw.shape[1]))))\n",
        "\n",
        "X_tfidf_1 = hstack((X_tfidf, trainX_tfidf_1))\n",
        "Xtst_tfidf_1 = hstack((X_tfidf_tst, tstX_tfidf_1))\n",
        "\n",
        "\n",
        "X_train_sparse = np.concatenate((X_tfidf_1.todense(), X_avg[:,None]), axis=1)\n",
        "X_test_sparse = np.concatenate((Xtst_tfidf_1.todense(), X_avg_tst[:,None]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQOEmhcZ9rd"
      },
      "source": [
        "X_train_sparse.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1n6et0XZ9rd"
      },
      "source": [
        "## Classify After Deep Text Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFfoBINGZ9re"
      },
      "source": [
        "### PCA and Logistic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epi_Kuhadkri"
      },
      "source": [
        "It is evident that the results of generating all these Tf-Idfs will explode the features space. As a workaround, we have used PCA to reduce this dimensionality. Then we apply our logictic regression model.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlQHmzW1f4xB"
      },
      "source": [
        "#####Without Average Word length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OephwflvgJc3"
      },
      "source": [
        "In this first model, we did not use the average word length feature as a predictor.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mrz177HZ9re",
        "outputId": "4584859c-86b6-4d45-dd16-9afc954d152b"
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "X_train_scaled = scaler.fit_transform(X_tfidf_1.todense())\n",
        "X_test_scaled = scaler.fit_transform(Xtst_tfidf_1.todense())\n",
        "\n",
        "\n",
        "# Define PCA\n",
        "pca = PCA(n_components=300)\n",
        "pca_1 = PCA(n_components=300)\n",
        "\n",
        "# Example on X_train_vec\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "print('Shape after PCA: ', X_train_pca.shape)\n",
        "print('Number of components: ', pca.n_components_)\n",
        "\n",
        "x_tst_pca = pca_1.fit_transform(X_test_scaled)\n",
        "print('Shape after PCA: ', x_tst_pca.shape)\n",
        "print('Number of components: ', pca_1.n_components_)\n",
        "\n",
        "\n",
        "# Fit model\n",
        "\n",
        "log_reg_s = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, random_state = 72)\n",
        "\n",
        "start = time.time()\n",
        "log_reg_s.fit(X_train_pca, y_train_dp)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "\n",
        "y_pred = log_reg_s.predict(x_tst_pca)\n",
        "y_pred\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after PCA:  (6471, 300)\n",
            "Number of components:  300\n",
            "Shape after PCA:  (1142, 300)\n",
            "Number of components:  300\n",
            "Time:  2.3724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brjbVW0Rgfjz"
      },
      "source": [
        "#####With All predictors: Average Word Length included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IzR3mzMguF_"
      },
      "source": [
        "Here we make use of the concatenated Tf-Ifd which includes average word length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwHY9c5Dpnzu",
        "outputId": "5843f3d8-9192-43a6-a0f3-c9e062b5356e"
      },
      "source": [
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
        "X_test_scaled = scaler.fit_transform( X_test_sparse)\n",
        "\n",
        "\n",
        "# Define PCA\n",
        "pca = PCA(n_components=100)\n",
        "pca_1 = PCA(n_components=100)\n",
        "\n",
        "# Example on X_train_vec\n",
        "X_train_pca_1 = pca.fit_transform(X_train_scaled)\n",
        "print('Shape after PCA: ', X_train_pca_1.shape)\n",
        "print('Number of components: ', pca.n_components_)\n",
        "\n",
        "x_tst_pca_1 = pca_1.fit_transform(X_test_scaled)\n",
        "print('Shape after PCA: ', x_tst_pca_1.shape)\n",
        "print('Number of components: ', pca_1.n_components_)\n",
        "\n",
        "\n",
        "# Fit model\n",
        "\n",
        "log_reg_s = LogisticRegressionCV(solver='saga', cv=5, max_iter=1200, random_state = 72)\n",
        "\n",
        "start = time.time()\n",
        "log_reg_s.fit(X_train_pca_1, y_train_dp)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "\n",
        "y_pred_1 = log_reg_s.predict(x_tst_pca_1)\n",
        "y_pred_1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after PCA:  (6471, 100)\n",
            "Number of components:  100\n",
            "Shape after PCA:  (1142, 100)\n",
            "Number of components:  100\n",
            "Time:  37.0618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTn0YHUhE8k"
      },
      "source": [
        "#####With All predictors: Average Word Length included: With different Hyperparameters and more number of Principal Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgKNsD8A3bv9",
        "outputId": "19a0493f-1572-4472-b912-9bba37431b2d"
      },
      "source": [
        "#With only Location and Text\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "X_train_scaled_1 = scaler.fit_transform(X_tfidf.todense())\n",
        "X_test_scaled_1 = scaler.fit_transform(X_tfidf_tst.todense())\n",
        "\n",
        " \n",
        "\n",
        "# Define PCA\n",
        "pca_2 = PCA(n_components=150)\n",
        "pca_3 = PCA(n_components=150)\n",
        "\n",
        "# Example on X_train_vec\n",
        "X_train_pca_2 = pca_2.fit_transform(X_train_scaled_1)\n",
        "print('Shape after PCA: ', X_train_pca_2.shape)\n",
        "print('Number of components: ', pca_2.n_components_)\n",
        "\n",
        "x_tst_pca_2 = pca_3.fit_transform(X_test_scaled_1)\n",
        "print('Shape after PCA: ', x_tst_pca_2.shape)\n",
        "print('Number of components: ', pca_3.n_components_)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "log_reg_rdc_2 = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, random_state = 72)\n",
        "\n",
        "log_reg_rdc_2.fit(X_train_pca_2, y_train_dp)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "\n",
        "y_pred_real_rdc_2 = log_reg_rdc_2.predict(x_tst_pca_2)\n",
        "y_pred_real_rdc_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after PCA:  (6471, 150)\n",
            "Number of components:  150\n",
            "Shape after PCA:  (1142, 150)\n",
            "Number of components:  150\n",
            "Time:  1.6223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfEnCmVhgyD"
      },
      "source": [
        "CONCLUSIONS:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7cqItaSl7QG"
      },
      "source": [
        "Given a chance to crosscheck the above predictions against the real ones, we strongly believe that our accuracies would have been improve. \n",
        "Yet, we could have, for instance, kept a part of our Trainset as a simulation of Test set.\n",
        "\n",
        "Another thing, we have realised, here, is that the chosen feature 'average word length' is not that meaningful. Because, after averaging both the distributions do not appear significantly different. Thus, it would have been better to use other features that we explored during our initial EDA, for instance, 'total character' or 'word length'.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_x3CTqWhjlx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}