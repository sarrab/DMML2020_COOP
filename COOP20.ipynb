{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COOP20.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarrab/DMML2020_COOP/blob/main/COOP20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkzLpJIbTlrv",
        "outputId": "7333f5cd-9c67-4e3f-b898-a6637d5e114e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URNFElJIZCNW"
      },
      "source": [
        "# Import required packages\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F65y_KSdY61u"
      },
      "source": [
        "import string\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,LinearRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm23FKT7Y_qu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1GoTZzmS_tU"
      },
      "source": [
        "# Load data:\n",
        "For the first part we import the data to get the firts informations about the data.  All the features in the dataset are:\n",
        "1-id\n",
        "2-keyword\n",
        "3-location\n",
        "4-text\n",
        "5-target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZgJGWPmUKza"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWyrTJl-TpO5",
        "outputId": "b9ce2be4-9d1b-4e58-d30d-c025f7a747f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/project/training_data.csv\")\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    keyword  ...                                               text target\n",
              "0   3738  destroyed  ...  Black Eye 9: A space battle occurred at Star O...      0\n",
              "1    853  bioterror  ...  #world FedEx no longer to transport bioterror ...      0\n",
              "2  10540  windstorm  ...  Reality Training: Train falls off elevated tra...      1\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlBMq9NU4bY",
        "outputId": "4952c9fa-7115-445e-9309-209cdcf9bc27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6471, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGiBSqVNUaDT"
      },
      "source": [
        "# Target values are composed of 0 and 1. Whether these twits are about a real disaster or not is the focus of the concern.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1bXn1mTUQvO",
        "outputId": "9f544d30-f092-4698-93f1-e3154ddfab2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.target.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPRysXDUw2X4",
        "outputId": "a263c327-e090-487f-a49d-a150db2467df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#baserate\n",
        "max(len(df[df[\"target\"] == 0]) / len(df), len(df[df[\"target\"] == 1]) / len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5719363313243703"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwrh27nkcfmy"
      },
      "source": [
        "##LOCATION-TARGET\n",
        "Is there a meaningful relation in the  the country level data. i.e: Can we expect that in certain places people are more prone to diffuse fake disaster news:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQVgtRr3ccfT",
        "outputId": "c5fcfbbf-b9f9-4b20-da51-b5311ebb0b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2=df[['location','target']].dropna()\n",
        "print(df2.shape)\n",
        "X=df['location'].dropna()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4330, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h0js_Gs6xc6",
        "outputId": "e1462244-1c48-4a52-b15b-4b541b238e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['location'].unique().shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2922,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP8xIEyIdiaI",
        "outputId": "078c94fb-32f0-4bb2-d9db-d0cc9a483892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Categorizing the location \n",
        "X.unique().shape # around 3000 unique locations. that would not be a good fature to classify the "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2921,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esp_TGnkfhA8"
      },
      "source": [
        "###KEYWORD-TARGET\n",
        "We can try to analyse the predictability of the keywords on the target. in another words, if for sertain types of disasters, whether people are more accurate, and for certain types people are more prone to hoax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZqbserofUF1",
        "outputId": "641a7ae6-d7ee-4709-a55b-6b08cecaf7b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df3=df[['keyword','target']].dropna()\n",
        "print(\"sjhape of df2:\", df2.shape)\n",
        "X=df3['keyword']\n",
        "y=df3['target']\n",
        "#How many unique keyword : 221\n",
        "print(\" unique cat in keyword:\", X.unique().shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sjhape of df2: (4330, 2)\n",
            " unique cat in keyword: (221,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN_FsxidkXoA"
      },
      "source": [
        "#a label encoding could be used to encode the keywords.\n",
        "myLabEn = LabelEncoder()\n",
        "df3_encoded = df3.apply(myLabEn.fit_transform) #\n",
        "#df3_encoded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plXPm6Gq9BxP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww9SjuIJkkdJ",
        "outputId": "932a9165-73f1-40ab-a05e-f69aa2236a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "#Data prepatation\n",
        "X=df3_encoded[['keyword']]\n",
        "y=df3.target\n",
        "\n",
        "#Splitting train and Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1741</th>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4259</th>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4237</th>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3893</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2433</th>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1679</th>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3346</th>\n",
              "      <td>173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1700</th>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6042</th>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5132 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      keyword\n",
              "1741      111\n",
              "4259       78\n",
              "4237       81\n",
              "3893        7\n",
              "2433      155\n",
              "...       ...\n",
              "973        88\n",
              "1679      120\n",
              "3346      173\n",
              "1700       42\n",
              "6042       70\n",
              "\n",
              "[5132 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOJoSCfYuoPP",
        "outputId": "72c88956-7f65-42fc-d621-7730223984c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Fitting\n",
        "LR=LogisticRegression(solver='lbfgs',  max_iter=1000,random_state=3)\n",
        "LR.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy of training set and test set\n",
        "LR.score(X_train, y_train)\n",
        "\n",
        "LR.score(X_test, y_test)\n",
        "print(\"Accuracy of train and test sets:\",LR.score(X_train, y_train), LR.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of train and test sets: 0.5693686671862821 0.5926791277258567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVX7-4y3u-hn",
        "outputId": "dd2e3e93-7b9c-48ae-eb19-680a960b5c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "LRcv=LogisticRegressionCV(solver='lbfgs', cv=5,  max_iter=1000,random_state=3)\n",
        "LRcv.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy of training set and test set\n",
        "LRcv.score(X_train, y_train)\n",
        "\n",
        "LRcv.score(X_test, y_test)\n",
        "print(\"Accuracy of train and test sets:\",LRcv.score(X_train, y_train), LRcv.score(X_test, y_test))\n",
        "\n",
        "#It seems the single relationsshipe between keyword and the target is not that meaningful. the accuracy rate is almost near to the base rate. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of train and test sets: 0.5693686671862821 0.5926791277258567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cY8qsf4Bp1x",
        "outputId": "e968150e-03ba-4d95-9888-147332e3f950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Predictions\n",
        "y_pred = LRcv.predict(X_test)\n",
        "#Confusion matrix\n",
        "confusion_matrix(y_test, y_pred)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJJJayCfYVbH",
        "outputId": "17f81102-9e7d-4a47-95ff-6c7b1f71c62f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install and update spaCy\n",
        "!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4MyFXK8YquB",
        "outputId": "4ab81c12-2f7d-481e-a931-1e4f0701bf32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the english language model\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ib6yhpAYq8l"
      },
      "source": [
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create a list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud6ASrN3YVjf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ArC1uJYVOp"
      },
      "source": [
        "#LEMMATIZE STOP WORD, PUNCTUATİONS.\n",
        "\n",
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "\n",
        "    mytokens=sp(sentence)\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    #remove RTs\n",
        "    #mytokens=[word for word in mytokens if word!='rt' or word[0]!='@' or word[0]!='#'] \n",
        "   \n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdDtzLWoZqdX"
      },
      "source": [
        "Train/test splitting\n",
        "Split the dataset into 80% training and 20% test set. Remeber to set the random seed to be 72."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7maexwFZaj_"
      },
      "source": [
        "X=df['text']\n",
        "ylabels=df['target']\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_lJsdHBaEa3"
      },
      "source": [
        "TF-IDF feature vectors\n",
        "create the TF-IDF feature vectors for the processed tweetes. These will construct you data features that you will use to train a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VElJnee4aI3s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}